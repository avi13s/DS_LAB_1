{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import col, udf, struct, from_json\nfrom pyspark.sql.types import DateType, TimestampType, FloatType\nimport pickle as pkl\nimport numpy as np\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd768a28-f711-4e78-bf71-ffb951404c60"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\ndef sigmoid(z):\n  z = np.array(z, dtype=np.float64)\n  return 1/(1+np.exp(-z))\n\nclass OurModel:\n  def __init__(self, input_size, output_size, hidden_size = 512, w1 = None, w2 = None):\n    self.hidden_size = hidden_size\n    self.output_size = output_size\n    self.w1 = w1\n    if w1 is None:\n      self.w1 = np.random.normal(size=(hidden_size,input_size),scale = 0.01)\n    self.w2 = w2\n    if w2 is None:\n      self.w2 = np.random.normal(size=(output_size,hidden_size),scale = 0.01)  # 10 like number of possible classifications\n    self.loss = 0 # at the start of every epoch should be set to 0\n\n  def forward(self, x):\n    x = np.array(x, ndmin=2)\n    self.z1 = np.dot(self.w1,x.T)\n    self.hidden = sigmoid(self.z1) \n    self.z2 = np.dot(self.w2,self.hidden)\n    y_hat = sigmoid(self.z2)\n    return y_hat\n\n\n  def backward(self, x, y, y_hat, lr = 0.005):\n    X = np.array(x, ndmin=2)\n    y = np.array(y, ndmin=2).T\n    batch_size = y.shape[1]\n\n    # looking for dl_dw2\n    dl_dy_hat = (2/batch_size)*(y_hat - y)\n    dy_hat_dz2 =  y_hat * (1- y_hat) # this is excatly the gradient of the sigmoid as dsig_dx = sig(x)*(1-sig(x))\n    dl_dw2 = np.dot(dl_dy_hat * dy_hat_dz2, self.hidden.T) # hidden.T is dz_dw2\n\n    # looking for dl_dw1\n    dl_dh = np.dot(self.w2.T, dl_dy_hat)\n    dh_dz1 = self.hidden * (1-self.hidden) \n    dl_dz1 = dl_dh * dh_dz1\n\n    # updating the weights accordingly\n    self.w1 -= lr * np.dot(dl_dz1, x) # x is dz1_dw1\n    self.w2 -= lr*dl_dw2\n\n# Initializing some lists that help keep track\nlosses = []\naccuracies = []\nys = []\ny_hats = []\nrecalls = []\nlabels_sum = []\nlabels_tables = []\n\ndef prep_forward_n_back(kafka_raw_df,y):\n  import numpy as np\n  \n  is_c = udf(lambda x: 1 if x==True else 0)  \n  get_hour_date = udf(lambda x: str(x)[:13])\n  \n  # Preprocessing\n  df_value = kafka_raw_df.withColumn(\"value\", col(\"value\").cast(\"string\")).select(\"value\").withColumn(\"json\", from_json( col(\"value\"), schema=schema)).select(\"json.*\")\n  df_value = df_value.withColumn(\"timestamp\", col(\"timestamp.$numberLong\").cast(\"bigint\")).withColumn(\"timestamp\", col(\"timestamp\")/1000).withColumn(\"date_timestamp\", col(\"timestamp\").cast(TimestampType()))\\\n  .withColumn(\"hourRounded\", get_hour_date(col(\"date_timestamp\")))\n  true_values = df_value.filter(col(\"congestion\") == True)\n  \n  # Balancing classes\n  sample = df_value.filter(col(\"congestion\") == False).sample(withReplacement=False, fraction=0.01)\n  df_value = true_values.union(sample)\n  labels_table = df_value.select(\"congestion\").withColumn(\"is_congestion\",is_c(col(\"congestion\"))).drop(\"congestion\")\n  \n  # Turning everything into pandas/numpy\n  df_value = df_value.select([\"currentHour\",\"latitude\",\"longitude\",\"hourRounded\"])\n  X = df_value.toPandas()\n  labels_tables.append(labels_table)\n  y = np.array(labels_table.toPandas()).astype('float')\n  temp_pd = temp.toPandas()\n  ys.append(y)\n  batch_size = len(X)\n  \n  \n  # Adding weather\n  X = X.merge(right=temp_pd, on='hourRounded', how='left').reset_index().drop('hourRounded',axis=1)\n  X = np.array(X)\n \n  # Just for keeping track on number of records from each label to see that it's balanced\n  batch_size = len(X)\n  congests = sum([1 if y[i,0] == 1 else 0 for i in range(batch_size)])\n  non_congests = sum([1 if y[i,0] == 0 else 0 for i in range(batch_size)])\n  labels_sum.append((congests,non_congests))\n  \n  # Training\n  y_hat = model.forward(X)\n  y_hats.append(y_hat)\n  batch_acc = sum([y[i,0] == round(y_hat[0,i]) for i in range(batch_size)])/(batch_size if batch_size!=0 else 1)\n  total_congestions = sum([1 if y[i,0] == 1 else 0 for i in range(batch_size)])\n  batch_recall = sum([1 if y[i,0] == 1 and round(y_hat[0,i]) == 1 else 0 for i in range(batch_size)])/(total_congestions if total_congestions!=0 else 1)\n  recalls.append(batch_recall)\n  accuracies.append(batch_acc)\n  loss = (1/batch_size)*np.linalg.norm(y.T-y_hat)\n  losses.append(loss)\n  pkl.dump(model.w1,open(\"/dbfs/FileStore/avi_maxim_models/w1.pkl\",'wb'))\n  pkl.dump(model.w2,open(\"/dbfs/FileStore/avi_maxim_models/w2.pkl\",'wb'))\n  model.backward(X, y, y_hat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Defining the NN model class and per-batch-function","showTitle":true,"inputWidgets":{},"nuid":"1456980e-60d3-45f7-8da6-188b4d925dcf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["model = OurModel(input_size=12,output_size=1,hidden_size =20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Initializing our NN","showTitle":true,"inputWidgets":{},"nuid":"f3f97631-c949-4e08-834e-225b542118c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["with open(\"/dbfs/FileStore/tables/schema.pkl\", \"rb\") as f:\n  schema = pkl.load(f)\n\nkafka_server = '10.0.0.30:9091'\n  \n\nkafka_raw_df = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", kafka_server) \\\n  .option(\"subscribePattern\", \"vehicleId_.*\") \\\n  .option(\"startingOffsets\", \"earliest\") \\\n  .option(\"maxOffsetsPerTrigger\", 50) \\\n  .load()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Initializing the stream","showTitle":true,"inputWidgets":{},"nuid":"14179c0d-19e4-4152-99be-356d9c325295"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"kafka_raw_df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"key","nullable":true,"type":"binary"},{"metadata":{},"name":"value","nullable":true,"type":"binary"},{"metadata":{},"name":"topic","nullable":true,"type":"string"},{"metadata":{},"name":"partition","nullable":true,"type":"integer"},{"metadata":{},"name":"offset","nullable":true,"type":"long"},{"metadata":{},"name":"timestamp","nullable":true,"type":"timestamp"},{"metadata":{},"name":"timestampType","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["get_hour_date = udf(lambda x: str(x)[:13])\nw = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/FileStore/tables/relevant_weather_updated.csv\")\nw = w.withColumn(\"hourRounded\", get_hour_date(col(\"date\")))\ntemp = w.select(\"hourRounded\", \"rain\", \"temp\", \"wetb\", \"dewpt\", \"vappr\", \"rhum\", \"msl\", \"vis\").groupby(\"hourRounded\").agg({\"rain\": \"avg\", \"temp\":\"avg\", \"wetb\":\"avg\", \"dewpt\":\"avg\", \"vappr\":\"avg\", \"rhum\":\"avg\", \"msl\":\"avg\", \"vis\":\"avg\"}).withColumnRenamed(\"avg(temp)\", \"temp\").withColumnRenamed(\"avg(msl)\", \"msl\").withColumnRenamed(\"avg(vis)\", \"vis\").withColumnRenamed(\"avg(rain)\", \"rain\").withColumnRenamed(\"avg(vappr)\", \"vappr\").withColumnRenamed(\"avg(rhum)\", \"rhum\").withColumnRenamed(\"avg(dewpt)\", \"dewpt\").withColumnRenamed(\"avg(wetb)\", \"wetb\")\ntemp.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Loading and pre-processing weather data","showTitle":true,"inputWidgets":{},"nuid":"db139ede-236c-4b04-a296-34767b2cd050"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"w","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"_c0","nullable":true,"type":"integer"},{"metadata":{},"name":"county","nullable":true,"type":"string"},{"metadata":{},"name":"station","nullable":true,"type":"string"},{"metadata":{},"name":"latitude","nullable":true,"type":"double"},{"metadata":{},"name":"longitude","nullable":true,"type":"double"},{"metadata":{},"name":"date","nullable":true,"type":"timestamp"},{"metadata":{},"name":"rain","nullable":true,"type":"double"},{"metadata":{},"name":"temp","nullable":true,"type":"double"},{"metadata":{},"name":"wetb","nullable":true,"type":"double"},{"metadata":{},"name":"dewpt","nullable":true,"type":"double"},{"metadata":{},"name":"vappr","nullable":true,"type":"double"},{"metadata":{},"name":"rhum","nullable":true,"type":"integer"},{"metadata":{},"name":"msl","nullable":true,"type":"double"},{"metadata":{},"name":"wdsp","nullable":true,"type":"integer"},{"metadata":{},"name":"wddir","nullable":true,"type":"integer"},{"metadata":{},"name":"sun","nullable":true,"type":"double"},{"metadata":{},"name":"vis","nullable":true,"type":"double"},{"metadata":{},"name":"clht","nullable":true,"type":"double"},{"metadata":{},"name":"clamt","nullable":true,"type":"double"},{"metadata":{},"name":"hourRounded","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"temp","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"hourRounded","nullable":true,"type":"string"},{"metadata":{},"name":"msl","nullable":true,"type":"double"},{"metadata":{},"name":"vis","nullable":true,"type":"double"},{"metadata":{},"name":"rain","nullable":true,"type":"double"},{"metadata":{},"name":"vappr","nullable":true,"type":"double"},{"metadata":{},"name":"temp","nullable":true,"type":"double"},{"metadata":{},"name":"rhum","nullable":true,"type":"double"},{"metadata":{},"name":"dewpt","nullable":true,"type":"double"},{"metadata":{},"name":"wetb","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n|  hourRounded|               msl|    vis|              rain|             vappr|              temp|             rhum|             dewpt|              wetb|\n+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n|2017-07-11 01|1007.4333333333334| 8500.0|0.4000000000000001|14.633333333333335|13.133333333333335|96.66666666666667|12.666666666666666|12.866666666666667|\n|2017-07-20 22|1003.3666666666667|20000.0|               0.0|11.166666666666666|12.299999999999999|78.33333333333333| 8.566666666666666|10.466666666666667|\n|2017-07-23 11|1013.1333333333333|20500.0|               0.0|15.299999999999999|              16.5|81.33333333333333|13.333333333333334|14.699999999999998|\n|2017-07-24 15|1019.2999999999998|47500.0|               0.0|15.833333333333334|22.433333333333334|             58.0|13.766666666666666|              17.4|\n|2017-07-25 17|1012.4333333333334|20000.0|               0.0|15.933333333333332|              18.0|             77.0|13.899999999999999|15.666666666666666|\n+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n  hourRounded|               msl|    vis|              rain|             vappr|              temp|             rhum|             dewpt|              wetb|\n+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\n2017-07-11 01|1007.4333333333334| 8500.0|0.4000000000000001|14.633333333333335|13.133333333333335|96.66666666666667|12.666666666666666|12.866666666666667|\n2017-07-20 22|1003.3666666666667|20000.0|               0.0|11.166666666666666|12.299999999999999|78.33333333333333| 8.566666666666666|10.466666666666667|\n2017-07-23 11|1013.1333333333333|20500.0|               0.0|15.299999999999999|              16.5|81.33333333333333|13.333333333333334|14.699999999999998|\n2017-07-24 15|1019.2999999999998|47500.0|               0.0|15.833333333333334|22.433333333333334|             58.0|13.766666666666666|              17.4|\n2017-07-25 17|1012.4333333333334|20000.0|               0.0|15.933333333333332|              18.0|             77.0|13.899999999999999|15.666666666666666|\n+-------------+------------------+-------+------------------+------------------+------------------+-----------------+------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["kafka_raw_df.writeStream.foreachBatch(prep_forward_n_back).start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Running on batches","showTitle":true,"inputWidgets":{},"nuid":"6c91e038-b0df-4c9e-ba0e-25c16ac67056"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f0a33a03b38&gt;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f0a33a03b38&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ef11985-a664-4375-93ad-ef0a0eee3dcb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Project_NN_try (1)","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3213778479524854}},"nbformat":4,"nbformat_minor":0}
